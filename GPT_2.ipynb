{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bitanb1999/TalentSumoAI/blob/main/GPT_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DEcHwVMf07mA",
        "outputId": "1d9a393c-56d5-4b97-bb1e-ff5b5c5e8bfc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.18.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.5.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.49)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.12.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "BZLKAMO4AD_R"
      },
      "outputs": [],
      "source": [
        "#for reproducability\n",
        "SEED = 34\n",
        "\n",
        "#maximum number of words in output text\n",
        "MAX_LEN = 70"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EeUMUsHxAP73"
      },
      "source": [
        "###I. Intro\n",
        "A language model is a machine learning model that can look at part of a sentence and predict the next word/sequence of words. Much like the autofill features on your iPhone/Android, GPT-2 is capable of next word prediction on a much larger and more sophisticated scale. For reference, the smallest available GPT-2 has 117 million parameters, whereas the largest one (invisible to the public) has over 1.5 billion parameters. The largest one available for public use is half the size of their main GPT-2 model\n",
        "\n",
        "ðŸ˜Š Transformers makes it very easy to import this model with both PyTorch and TensorFlow - in this notebook we will be using TensorFlow but it is just as easy in PyTorch. Both the model and its Tokenizer can be imported from the transformers library that anyone can get by typing !pip install transformers. Let's see just how simple it is to generate text with a neural network. We begin with our input sequence:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "yr7wFVZfALoD"
      },
      "outputs": [],
      "source": [
        "input_sequence = \"I am enthusiastic,diligent and hardowking. I am very passionate about my work and am a very good communicator. I \""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZQTw2_i9AdXC",
        "outputId": "c54ab006-74b3-433d-bff3-6cfaa1dc26e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
            "\n",
            "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at gpt2-large.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"tfgpt2lm_head_model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " transformer (TFGPT2MainLaye  multiple                 774030080 \n",
            " r)                                                              \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 774,030,080\n",
            "Trainable params: 774,030,080\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "#get transformers\n",
        "from transformers import TFGPT2LMHeadModel, GPT2Tokenizer\n",
        "\n",
        "#get large GPT2 tokenizer and GPT2 model\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2-large\")\n",
        "GPT2 = TFGPT2LMHeadModel.from_pretrained(\"gpt2-large\", pad_token_id=tokenizer.eos_token_id)\n",
        "GPT2.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "HaMJwfJsAkQM"
      },
      "outputs": [],
      "source": [
        "#get deep learning basics\n",
        "import tensorflow as tf\n",
        "tf.random.set_seed(SEED)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "umtK_l2TCYuQ",
        "outputId": "45b22dcc-5d3f-4ba4-a8dd-563fef337790"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Output:\n",
            "--------------------------------------------------\n",
            "0: I am enthusiastic,diligent and hardowking. I am very passionate about my work and am a very good communicator. I Â have a great sense of humor, and I love to make people laugh. If you have any questions, please feel free to contact me.\n",
            "1: I am enthusiastic,diligent and hardowking. I am very passionate about my work and am a very good communicator. I Â have a great sense of humor, and I love to make people laugh. If you have any questions, please feel free to send me an e-mail.\n",
            "2: I am enthusiastic,diligent and hardowking. I am very passionate about my work and am a very good communicator. I Â have a great sense of humor, and I love to make people laugh. If you have any questions, please feel free to contact me. Thank you for your time.\n",
            "3: I am enthusiastic,diligent and hardowking. I am very passionate about my work and am a very good communicator. I Â have a great sense of humor, and I love to make people laugh. If you have any questions, please feel free to contact me. Thank you for visiting my website.\n",
            "4: I am enthusiastic,diligent and hardowking. I am very passionate about my work and am a very good communicator. I Â have a great sense of humor, and I love to make people laugh. If you have any questions, please feel free to contact me. Thank you for visiting my website!\n"
          ]
        }
      ],
      "source": [
        "# encode context the generation is conditioned on\n",
        "input_ids = tokenizer.encode(input_sequence, return_tensors='tf')\n",
        "# set return_num_sequences > 1\n",
        "beam_outputs = GPT2.generate(\n",
        "    input_ids, \n",
        "    max_length = MAX_LEN, \n",
        "    num_beams = 5, \n",
        "    no_repeat_ngram_size = 2, \n",
        "    num_return_sequences = 5, \n",
        "    early_stopping = True\n",
        ")\n",
        "\n",
        "print('')\n",
        "print(\"Output:\\n\" + 50 * '-')\n",
        "\n",
        "# now we have 3 output sequences\n",
        "for i, beam_output in enumerate(beam_outputs):\n",
        "      print(\"{}: {}\".format(i, tokenizer.decode(beam_output, skip_special_tokens=True)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "CeVQHOuzCv8t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38249c19-3249-4c8b-908a-4672615ccdf7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "I am enthusiastic,diligent and hardowking. I am very passionate about my work and am a very good communicator. I _________________________________________\" I have boiled my stories down to their essence and the emotion is flowing all around me.\n",
            "\n",
            "\n",
            "I am a semi-professional photographer and writer. My skills are mostly focused on family\n"
          ]
        }
      ],
      "source": [
        "# use temperature to decrease the sensitivity to low probability candidates\n",
        "sample_output = GPT2.generate(\n",
        "                             input_ids, \n",
        "                             do_sample = True, \n",
        "                             max_length = MAX_LEN, \n",
        "                             top_k = 0, \n",
        "                             temperature = 0.8\n",
        ")\n",
        "\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "print(tokenizer.decode(sample_output[0], skip_special_tokens = True))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#sample only from 80% most likely words\n",
        "sample_output = GPT2.generate(\n",
        "                             input_ids, \n",
        "                             do_sample = True, \n",
        "                             max_length = MAX_LEN, \n",
        "                             top_p = 0.8, \n",
        "                             top_k = 0\n",
        ")\n",
        "\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "print(tokenizer.decode(sample_output[0], skip_special_tokens = True), '...')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dbCc6aKQGv4L",
        "outputId": "b3de2461-f6e8-4243-f36e-9faa67589c2f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "I am enthusiastic,diligent and hardowking. I am very passionate about my work and am a very good communicator. I ive always done this work with my whole heart and I will always do it and i am no exceptions!I love to do any job that im doing. And it is my passion and desire to make the ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#combine both sampling techniques\n",
        "sample_outputs = GPT2.generate(\n",
        "                              input_ids,\n",
        "                              do_sample = True, \n",
        "                              max_length = 2*MAX_LEN,                              #to test how long we can generate and it be coherent\n",
        "                              #temperature = .7,\n",
        "                              top_k = 50, \n",
        "                              top_p = 0.85, \n",
        "                              num_return_sequences = 5\n",
        ")\n",
        "\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "for i, sample_output in enumerate(sample_outputs):\n",
        "    print(\"{}: {}...\".format(i, tokenizer.decode(sample_output, skip_special_tokens = True)))\n",
        "    print('')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VESkcwCdH2Uw",
        "outputId": "c563cc1b-4e53-40e5-b8b2-139315da9de7"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "0: I am enthusiastic,diligent and hardowking. I am very passionate about my work and am a very good communicator. I Â would love to be your leader!...\n",
            "\n",
            "1: I am enthusiastic,diligent and hardowking. I am very passionate about my work and am a very good communicator. I ive been an active member of the Church for many years and have been a member of the Priesthood for a couple of years. I am also an active member of the Church Youth, the Young Women, the Young Men, the Relief Society and the Young Men's Mutual Improvement Associations. I have a good understanding of the social and cultural needs of the community and have been working with these groups for many years.\n",
            "\n",
            "I believe that I have a genuine and authentic love for the Church and I am committed to being a dedicated, faithful, and faithful...\n",
            "\n",
            "2: I am enthusiastic,diligent and hardowking. I am very passionate about my work and am a very good communicator. I _____.I _____. I'm a good person. I try to do the right thing in everything that I do and have a great attitude about it. I am ____. I am a good friend. I'm honest and straightforward, very loyal and have a very positive attitude. I am _____. I'm honest and trustworthy, honest and trustworthy, and honest and trustworthy. I'm _____. I like _____ and I like to work with people who are like me, and I like to think that if I were to _____...\n",
            "\n",
            "3: I am enthusiastic,diligent and hardowking. I am very passionate about my work and am a very good communicator. I ive worked hard for years to get this position and i am very proud to be part of it. I am extremely grateful to my teammates and to the team at the team. You guys are amazing!...\n",
            "\n",
            "4: I am enthusiastic,diligent and hardowking. I am very passionate about my work and am a very good communicator. I ike to help people and I like to do work that helps people, especially when it is something I am passionate about....\n",
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "GPT-2.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Audio_Generation (1).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Requirements"
      ],
      "metadata": {
        "id": "LzkaiOj_Xjny"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fuzzywuzzy\n",
        "!pip install imageio==2.4.1\n",
        "!pip install pygame"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-dw1o9J-5Bws",
        "outputId": "91cdaab9-b2d9-43e5-c690-d8bb1d1f825d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting fuzzywuzzy\n",
            "  Downloading fuzzywuzzy-0.18.0-py2.py3-none-any.whl (18 kB)\n",
            "Installing collected packages: fuzzywuzzy\n",
            "Successfully installed fuzzywuzzy-0.18.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting imageio==2.4.1\n",
            "  Downloading imageio-2.4.1.tar.gz (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 4.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from imageio==2.4.1) (1.21.6)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from imageio==2.4.1) (7.1.2)\n",
            "Building wheels for collected packages: imageio\n",
            "  Building wheel for imageio (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for imageio: filename=imageio-2.4.1-py3-none-any.whl size=3303885 sha256=6b22ea68a99d79dea30244bf3e9dc3d55fb48a2a7e4029ad841f932bf9fe113d\n",
            "  Stored in directory: /root/.cache/pip/wheels/46/20/07/7bb9c8c44e6ec2efa60fd0e6280094f53f65f41767ef69a5ee\n",
            "Successfully built imageio\n",
            "Installing collected packages: imageio\n",
            "  Attempting uninstall: imageio\n",
            "    Found existing installation: imageio 2.9.0\n",
            "    Uninstalling imageio-2.9.0:\n",
            "      Successfully uninstalled imageio-2.9.0\n",
            "Successfully installed imageio-2.4.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pygame\n",
            "  Downloading pygame-2.1.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (21.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 21.8 MB 2.5 MB/s \n",
            "\u001b[?25hInstalling collected packages: pygame\n",
            "Successfully installed pygame-2.1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PyQt5 nltk pyLDAvis\n",
        "!pip install numba==0.48\n",
        "!pip install librosa==0.6\n",
        "import librosa\n",
        "!pip install amfm_decompy aubio pydub\n",
        "!pip install Wave moviepy==0.2.1.1 SpeechRecognition\n",
        "!pip install sentence-transformers\n",
        "!pip install wordninja"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "R5v6JGlcXhit",
        "outputId": "673ae29c-bc63-4d8c-896d-accc4e93d769"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting PyQt5\n",
            "  Downloading PyQt5-5.15.7-cp37-abi3-manylinux1_x86_64.whl (8.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 8.4 MB 5.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.7)\n",
            "Collecting pyLDAvis\n",
            "  Downloading pyLDAvis-3.3.1.tar.gz (1.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.7 MB 58.7 MB/s \n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting PyQt5-sip<13,>=12.11\n",
            "  Downloading PyQt5_sip-12.11.0-cp37-cp37m-manylinux1_x86_64.whl (344 kB)\n",
            "\u001b[K     |████████████████████████████████| 344 kB 48.8 MB/s \n",
            "\u001b[?25hCollecting PyQt5-Qt5>=5.15.0\n",
            "  Downloading PyQt5_Qt5-5.15.2-py3-none-manylinux2014_x86_64.whl (59.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 59.9 MB 91 kB/s \n",
            "\u001b[?25hRequirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk) (1.1.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.7/dist-packages (from nltk) (2022.6.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk) (7.1.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk) (4.64.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (1.7.3)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (1.21.6)\n",
            "Collecting sklearn\n",
            "  Downloading sklearn-0.0.tar.gz (1.1 kB)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (0.16.0)\n",
            "Collecting funcy\n",
            "  Downloading funcy-1.17-py2.py3-none-any.whl (33 kB)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (3.6.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (2.11.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (57.4.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (1.0.2)\n",
            "Requirement already satisfied: numexpr in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (2.8.3)\n",
            "Requirement already satisfied: pandas>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (1.3.5)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.2.0->pyLDAvis) (2022.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.2.0->pyLDAvis) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=1.2.0->pyLDAvis) (1.15.0)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim->pyLDAvis) (5.2.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->pyLDAvis) (2.0.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from numexpr->pyLDAvis) (21.3)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->numexpr->pyLDAvis) (3.0.9)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->pyLDAvis) (3.1.0)\n",
            "Building wheels for collected packages: pyLDAvis, sklearn\n",
            "  Building wheel for pyLDAvis (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyLDAvis: filename=pyLDAvis-3.3.1-py2.py3-none-any.whl size=136898 sha256=e9da1f5c2bac6e5027d201614b1b09207c7f506d1850b8a825dd4451f0c7151f\n",
            "  Stored in directory: /root/.cache/pip/wheels/c9/21/f6/17bcf2667e8a68532ba2fbf6d5c72fdf4c7f7d9abfa4852d2f\n",
            "  Building wheel for sklearn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sklearn: filename=sklearn-0.0-py2.py3-none-any.whl size=1310 sha256=191039866516fb502d3fdaff99f5a47114805f5f82a387e5656c0aea713e10da\n",
            "  Stored in directory: /root/.cache/pip/wheels/46/ef/c3/157e41f5ee1372d1be90b09f74f82b10e391eaacca8f22d33e\n",
            "Successfully built pyLDAvis sklearn\n",
            "Installing collected packages: sklearn, PyQt5-sip, PyQt5-Qt5, funcy, PyQt5, pyLDAvis\n",
            "Successfully installed PyQt5-5.15.7 PyQt5-Qt5-5.15.2 PyQt5-sip-12.11.0 funcy-1.17 pyLDAvis-3.3.1 sklearn-0.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting numba==0.48\n",
            "  Downloading numba-0.48.0-1-cp37-cp37m-manylinux2014_x86_64.whl (3.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.5 MB 5.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba==0.48) (57.4.0)\n",
            "Collecting llvmlite<0.32.0,>=0.31.0dev0\n",
            "  Downloading llvmlite-0.31.0-cp37-cp37m-manylinux1_x86_64.whl (20.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 20.2 MB 1.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.7/dist-packages (from numba==0.48) (1.21.6)\n",
            "Installing collected packages: llvmlite, numba\n",
            "  Attempting uninstall: llvmlite\n",
            "    Found existing installation: llvmlite 0.39.0\n",
            "    Uninstalling llvmlite-0.39.0:\n",
            "      Successfully uninstalled llvmlite-0.39.0\n",
            "  Attempting uninstall: numba\n",
            "    Found existing installation: numba 0.56.0\n",
            "    Uninstalling numba-0.56.0:\n",
            "      Successfully uninstalled numba-0.56.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "resampy 0.4.0 requires numba>=0.53, but you have numba 0.48.0 which is incompatible.\u001b[0m\n",
            "Successfully installed llvmlite-0.31.0 numba-0.48.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting librosa==0.6\n",
            "  Downloading librosa-0.6.0.tar.gz (1.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5 MB 4.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: audioread>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa==0.6) (3.0.0)\n",
            "Requirement already satisfied: numpy>=1.8.0 in /usr/local/lib/python3.7/dist-packages (from librosa==0.6) (1.21.6)\n",
            "Requirement already satisfied: scipy>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from librosa==0.6) (1.7.3)\n",
            "Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from librosa==0.6) (1.0.2)\n",
            "Requirement already satisfied: joblib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from librosa==0.6) (1.1.0)\n",
            "Requirement already satisfied: decorator>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa==0.6) (4.4.2)\n",
            "Requirement already satisfied: six>=1.3 in /usr/local/lib/python3.7/dist-packages (from librosa==0.6) (1.15.0)\n",
            "Requirement already satisfied: resampy>=0.2.0 in /usr/local/lib/python3.7/dist-packages (from librosa==0.6) (0.4.0)\n",
            "Collecting numba>=0.53\n",
            "  Downloading numba-0.56.0-cp37-cp37m-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (3.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.5 MB 54.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba>=0.53->resampy>=0.2.0->librosa==0.6) (57.4.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from numba>=0.53->resampy>=0.2.0->librosa==0.6) (4.12.0)\n",
            "Collecting llvmlite<0.40,>=0.39.0dev0\n",
            "  Downloading llvmlite-0.39.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 34.6 MB 1.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn!=0.19.0,>=0.14.0->librosa==0.6) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->numba>=0.53->resampy>=0.2.0->librosa==0.6) (4.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->numba>=0.53->resampy>=0.2.0->librosa==0.6) (3.8.1)\n",
            "Building wheels for collected packages: librosa\n",
            "  Building wheel for librosa (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for librosa: filename=librosa-0.6.0-py3-none-any.whl size=1553494 sha256=86cc3e1d6d43b152b8347a56ed002b571a93c0a3697591c56c56885c4e070c62\n",
            "  Stored in directory: /root/.cache/pip/wheels/c3/b3/35/7d46f91ae587ddc98ef185534386c1830ed6773a93a8da085d\n",
            "Successfully built librosa\n",
            "Installing collected packages: llvmlite, numba, librosa\n",
            "  Attempting uninstall: llvmlite\n",
            "    Found existing installation: llvmlite 0.31.0\n",
            "    Uninstalling llvmlite-0.31.0:\n",
            "      Successfully uninstalled llvmlite-0.31.0\n",
            "  Attempting uninstall: numba\n",
            "    Found existing installation: numba 0.48.0\n",
            "    Uninstalling numba-0.48.0:\n",
            "      Successfully uninstalled numba-0.48.0\n",
            "  Attempting uninstall: librosa\n",
            "    Found existing installation: librosa 0.8.1\n",
            "    Uninstalling librosa-0.8.1:\n",
            "      Successfully uninstalled librosa-0.8.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "kapre 0.3.7 requires librosa>=0.7.2, but you have librosa 0.6.0 which is incompatible.\u001b[0m\n",
            "Successfully installed librosa-0.6.0 llvmlite-0.39.0 numba-0.56.0\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-ee5d4da379e9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install numba==0.48'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install librosa==0.6'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mlibrosa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install amfm_decompy aubio pydub'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install Wave moviepy==0.2.1.1 SpeechRecognition'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/librosa/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# And all the librosa sub-modules\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbeat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdecompose\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/librosa/core/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    100\u001b[0m \"\"\"\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mtime_frequency\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m  \u001b[0;31m# pylint: disable=wildcard-import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0maudio\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m  \u001b[0;31m# pylint: disable=wildcard-import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mspectrum\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m  \u001b[0;31m# pylint: disable=wildcard-import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/librosa/core/time_frequency.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mParameterError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m __all__ = ['frames_to_samples', 'frames_to_time',\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/librosa/util/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mfiles\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m  \u001b[0;31m# pylint: disable=wildcard-import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdeprecation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m  \u001b[0;31m# pylint: disable=wildcard-import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdecorators\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/librosa/util/decorators.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdecorator\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdecorator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnumba\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecorators\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjit\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0moptional_jit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0m__all__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'moved'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'deprecated'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'optional_jit'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'numba.decorators'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim docx2txt\n",
        "from gensim.summarization.summarizer import summarize\n",
        "from gensim.summarization import keywords# Import the library\n",
        "# to convert MSword doc to txt for processing.\n",
        "import docx2txt\n",
        "!pip install textract PyPDF2\n",
        "import textract\n",
        "import PyPDF2\n",
        "!pip install jsonlines\n",
        "!pip install pyyaml==5.4.1"
      ],
      "metadata": {
        "id": "WoB76e4bdTUT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#spacy\n",
        "import spacy\n",
        "from spacy.pipeline import EntityRuler\n",
        "from spacy.lang.en import English\n",
        "from spacy.tokens import Doc\n",
        "\n",
        "#gensim\n",
        "import gensim\n",
        "from gensim import corpora\n",
        "\n",
        "#Visualization\n",
        "from spacy import displacy\n",
        "import pyLDAvis.gensim_models\n",
        "from wordcloud import WordCloud\n",
        "import plotly.express as px\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#Data loading/ Data manipulation\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import jsonlines\n",
        "\n",
        "#nltk\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download(['stopwords','wordnet'])\n",
        "\n",
        "#warning\n",
        "import warnings \n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "X4HuUHubThdU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/pandmi/jobzilla_ai/main/jz_skill_patterns.jsonl"
      ],
      "metadata": {
        "id": "UNyTVJjig284"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import en_core_web_sm\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "skill_pattern_path = \"/content/jz_skill_patterns.jsonl\"\n",
        "ruler = nlp.add_pipe(\"entity_ruler\")\n",
        "ruler.from_disk(skill_pattern_path)\n",
        "nlp.pipe_names"
      ],
      "metadata": {
        "id": "LwIYVUKBXlfb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Input"
      ],
      "metadata": {
        "id": "LTbNizqpXo9c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['GOOGLE_CLOUD_PROJECT']=\"august-cirrus-306813\""
      ],
      "metadata": {
        "id": "293YbvFlLZVb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MRXDBOwLe_S2"
      },
      "outputs": [],
      "source": [
        "from google.colab import auth\n",
        "import gspread\n",
        "from google.auth import default\n",
        "#autenticating to google\n",
        "auth.authenticate_user()\n",
        "creds, _ = default()\n",
        "gc = gspread.authorize(creds)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "#defining my worksheet\n",
        "worksheet = gc.open('Outsourcing Tracker').sheet1\n",
        "#get_all_values gives a list of rows\n",
        "rows = worksheet.get_all_values()\n",
        "#Convert to a DataFrame \n",
        "df = pd.DataFrame(rows)\n",
        "df.columns = df.iloc[0]\n",
        "df = df.iloc[1:]\n",
        "df.head()"
      ],
      "metadata": {
        "id": "xM6Jdl8UqvGn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(7):\n",
        "  dir= \"question_answers\"+str(i+1)\n",
        "  %mkdir $dir\n",
        "  %cd $dir\n",
        "  file_id=df.iloc[i]['Response'].split(\"/\")[-2]\n",
        "  !gdown $file_id\n",
        "  %cd ../ "
      ],
      "metadata": {
        "id": "ucScprmZzMq7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Calculations:"
      ],
      "metadata": {
        "id": "7Qs2YVL4ZrN9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#ideal answers\n",
        "ideal={}\n",
        "for i in range(len(df['Ideal_answer'])-1):\n",
        "  ideal[i+1]=df.iloc[i]['Ideal_answer']"
      ],
      "metadata": {
        "id": "5ynFaH_7SBQz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ideal"
      ],
      "metadata": {
        "id": "n_KoUKLNW4nK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#content_scoring flag\n",
        "flag=df['Content_Scoring']\n",
        "dat_format=df['Answer_Format']\n",
        "base_flag=df['Base_Scoring ']\n",
        "id={}\n",
        "labels=['Interaction_ID','Candidate_ID','Test_ID']\n",
        "for i in labels:\n",
        "  id[i]=df.iloc[0][i]"
      ],
      "metadata": {
        "id": "d7U-z2mYYJl9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "flag"
      ],
      "metadata": {
        "id": "5uFEPENg3P5Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_text_from_pdf(file):\n",
        "    '''Opens and reads in a PDF file from path'''\n",
        "    \n",
        "    pdfFileObj = open(file, 'rb')\n",
        "      \n",
        "    # creating a pdf reader object\n",
        "    pdfReader = PyPDF2.PdfFileReader(pdfFileObj)\n",
        "      \n",
        "    # printing number of pages in pdf file\n",
        "    page_count = pdfReader.getNumPages()\n",
        "      \n",
        "    # creating a page object\n",
        "    pageObj = pdfReader.getPage(0)\n",
        "      \n",
        "    # extracting text from page\n",
        "    text = [pdfReader.getPage(i).extractText() for i in range(page_count)]\n",
        "      \n",
        "    # closing the pdf file object\n",
        "    pdfFileObj.close()\n",
        "    \n",
        "    \n",
        "    \n",
        "    return str(text).replace(\"\\\\n\", \"\")"
      ],
      "metadata": {
        "id": "PsPo7sr_d0TA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import en_core_web_sm\n",
        "\n",
        "# Load pre-trained English language model\n",
        "#nlp = en_core_web_sm.load()\n",
        "\n",
        "def create_tokenized_texts_list():\n",
        "    '''Create two lists, one with the names of the candidate and one with the tokenized \n",
        "       resume texts extracted from either a .pdf or .doc'''\n",
        "    resume_texts= ''\n",
        "    \n",
        "    # Loop over the contents of the directory containing the resumes, filtering by .pdf or .doc(x)\n",
        "    resume=os.listdir('/content/question_answers7/')[0]\n",
        "\n",
        "    # Read in every resume with pdf extension in the directory\n",
        "    resume_texts=(extract_text_from_pdf('/content/question_answers7/' + resume))\n",
        "    \n",
        "        \n",
        "    return resume_texts, resume\n"
      ],
      "metadata": {
        "id": "c-iFjhFOeC-0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_skills(text):\n",
        "    doc = nlp(text)\n",
        "    myset = []\n",
        "    subset = []\n",
        "    for ent in doc.ents:\n",
        "        if ent.label_ == \"SKILL\":\n",
        "            subset.append(ent.text)\n",
        "    myset.append(subset)\n",
        "    return subset\n",
        "\n",
        "\n",
        "def unique_skills(x):\n",
        "    return list(set(x))"
      ],
      "metadata": {
        "id": "41buMeRzgXLN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import wordninja\n",
        "nltk.download('omw-1.4')\n",
        "def clean_resume():\n",
        "  texts,names=create_tokenized_texts_list()\n",
        "  texts=' '.join(texts.split(\"\\\\uf\"))\n",
        "  clean = []\n",
        "\n",
        "  review = re.sub(\n",
        "      '(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|^rt|http.+?\"',\n",
        "      \" \",\n",
        "      texts,\n",
        "  )\n",
        "  review = review.lower()\n",
        "  review = review.split()\n",
        "  lm = WordNetLemmatizer()\n",
        "  review = [\n",
        "      lm.lemmatize(word)\n",
        "      for word in review\n",
        "      if not word in set(stopwords.words(\"english\"))\n",
        "  ]\n",
        "  review = \" \".join(review)\n",
        "  clean=' '.join(wordninja.split(review))\n",
        "  return clean"
      ],
      "metadata": {
        "id": "jRh5TJoG1edF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from fuzzywuzzy import fuzz\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "# get the match percentage\n",
        "def resume_score():\n",
        "  cleaned_resume=clean_resume()\n",
        "  job_skills=df.iloc[0]['Description']\n",
        "  text_list = [cleaned_resume,job_skills]\n",
        "  cv = CountVectorizer()\n",
        "  count_matrix = cv.fit_transform(text_list)\n",
        "  matchPercentage = cosine_similarity(count_matrix)[0][1] * 100\n",
        "  matchPercentage = round(matchPercentage, 2)\n",
        "  mp2=round(((fuzz.token_sort_ratio(cleaned_resume.lower(),job_skills.lower()))/len(job_skills.split(\" \")))*100)\n",
        "\n",
        "\n",
        "  return mp2 if mp2>matchPercentage else matchPercentage\n"
      ],
      "metadata": {
        "id": "D3IrSQ2uhiZ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resume_sc=resume_score()\n",
        "resume_sc"
      ],
      "metadata": {
        "id": "qli_kNRQ2TJb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def startConvertion(path = 'sample.wav',lang = 'en-IN'):\n",
        "    with sr.AudioFile(path) as source:\n",
        "        r = sr.Recognizer()\n",
        "        print('Fetching File')\n",
        "        audio_text = r.listen(source,timeout=6000, phrase_time_limit=6000)\n",
        "        # recoginize_() method will throw a request error if the API is unreachable, hence using exception handling\n",
        "        try:\n",
        "        \n",
        "            # using google speech recognition\n",
        "            print('Converting audio transcripts into text ...')\n",
        "            text = r.recognize_google(audio_text)\n",
        "            print(text)\n",
        "            return text\n",
        "    \n",
        "        except:\n",
        "            print('Sorry.. run again...')"
      ],
      "metadata": {
        "id": "wWarLstxZ3Xk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transcript Grammer Score"
      ],
      "metadata": {
        "id": "EB4oDwbR3QLe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U git+https://github.com/PrithivirajDamodaran/Gramformer.git\n",
        "from gramformer import Gramformer\n",
        "import torch\n",
        "!python -m spacy download en\n",
        "import spacy\n",
        "spacy.load(\"en\")"
      ],
      "metadata": {
        "id": "ITShtQX324kh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def set_seed(seed):\n",
        "  torch.manual_seed(seed)\n",
        "  if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "set_seed(1212)\n",
        "gf = Gramformer(models = 1, use_gpu=False)"
      ],
      "metadata": {
        "id": "v7PO9LBn3uyo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Grammer_score(net_text):\n",
        "  percent_error = []\n",
        "  s = 0\n",
        "  for i in net_text:\n",
        "    influent_sentence = i\n",
        "    corrected_sentences = gf.correct(influent_sentence, max_candidates=1)\n",
        "    print(\"[Input] \", influent_sentence)\n",
        "    for corrected_sentence in corrected_sentences:\n",
        "      print(\"corrected sentence: \"+corrected_sentence)\n",
        "      \n",
        "    percent_error.append((len(gf.get_edits(influent_sentence, corrected_sentence))/len(influent_sentence.split()))*100)\n",
        "    s+= percent_error\n",
        "  return percent_error , s/(len(net_text))"
      ],
      "metadata": {
        "id": "PQUHE5Se32YP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Grammer_score(\"hi, how is you\")"
      ],
      "metadata": {
        "id": "Z6KtiAHR5IoX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd\n",
        "import wave, math, contextlib\n",
        "import speech_recognition as sr\n",
        "#import imageio-ffmpeg\n",
        "from moviepy.editor import AudioFileClip\n",
        "\n",
        "#returns pace,duration,text,filenames\n",
        "def audio_read(path):\n",
        "  net_text=[]\n",
        "  net_word_count=[]\n",
        "  net_total_duration=[]\n",
        "  net_transcribed_audio_file_name=[]\n",
        "  for i in range(len(df['Response'])-1):\n",
        "    transcribed_audio_file_name = \"transcribed_speech\"+str(i+1)+\".wav\"\n",
        "    zoom_video_file_name = path[i]\n",
        "    audioclip = AudioFileClip(zoom_video_file_name)\n",
        "    audioclip.write_audiofile(transcribed_audio_file_name)\n",
        "    with contextlib.closing(wave.open(transcribed_audio_file_name,'r')) as f:\n",
        "        frames = f.getnframes()\n",
        "        rate = f.getframerate()\n",
        "        duration = frames / float(rate)\n",
        "    total_duration = math.ceil(duration / 60)\n",
        "    r = sr.Recognizer()\n",
        "    with open('transcription.txt', 'w') as f:\n",
        "      print(f.write(startConvertion(transcribed_audio_file_name)))\n",
        "    with open('transcription.txt', 'r') as f:\n",
        "      text=f.read()\n",
        "    word_count_tex=len(text.split(\" \"))/(total_duration/2);\n",
        "    #print(\"Words per minute are:\",word_count_tex)\n",
        "    net_text.append(text)\n",
        "    net_word_count.append(word_count_tex)\n",
        "    net_total_duration.append(total_duration)\n",
        "    net_transcribed_audio_file_name.append(transcribed_audio_file_name)\n",
        "  return net_text,net_word_count,net_total_duration,net_transcribed_audio_file_name"
      ],
      "metadata": {
        "id": "mBWCbHz-Q2Rx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#storing all the data\n",
        "path=[]\n",
        "for i in range(len(df['Response'])-1):\n",
        "  dir='/content/question_answers'+str(i+1)+'/'\n",
        "  newfile=os.listdir(dir)[0]\n",
        "  new_dir=dir+newfile\n",
        "  path.append(new_dir)"
      ],
      "metadata": {
        "id": "Xh9OJYkhZeLX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path"
      ],
      "metadata": {
        "id": "_vLjy5wyaiXh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net_text,net_word_count,net_total_duration,net_transcribed_audio_file_name=audio_read(path)"
      ],
      "metadata": {
        "id": "krYGE37Rajel"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net_text[0],net_transcribed_audio_file_name[0],net_total_duration[0]"
      ],
      "metadata": {
        "id": "xKsN6Oj6aryg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "#print(stopwords.words('english'))\n",
        "from nltk.tokenize import word_tokenize\n",
        "stop_words = set(stopwords.words('english'))\n",
        "def filtered_words(net_text):\n",
        "  net_filtered=[]\n",
        "  net_filtered_unique=[]\n",
        "  for i in net_text:\n",
        "    word_tokens = word_tokenize(i)\n",
        "    filtered_sentence = []\n",
        "    for w in word_tokens:\n",
        "      if w not in stop_words:\n",
        "        filtered_sentence.append(w)\n",
        "    net_filtered.append(filtered_sentence)\n",
        "    filtered_sentence_unique=list(pd.Series(filtered_sentence).unique())\n",
        "    net_filtered_unique.append(filtered_sentence_unique)\n",
        "    #print(word_tokens)\n",
        "    #print(filtered_sentence)\n",
        "  return net_filtered,net_filtered_unique"
      ],
      "metadata": {
        "id": "RtZ9ZonkbHOw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net_filtered,net_filtered_unique=filtered_words(net_text)"
      ],
      "metadata": {
        "id": "KsKjzbgSsA13"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from operator import itemgetter\n",
        "import math\n",
        "def total_length(net_text):\n",
        "  net_total_words=[]\n",
        "  for text in net_text:\n",
        "    total_words = text.split()\n",
        "    total_word_length = len(total_words)\n",
        "    net_total_words.append(total_word_length)\n",
        "  #print(total_word_length)\n",
        "  return net_total_words"
      ],
      "metadata": {
        "id": "HbjSa9FitHBs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net_total_length=total_length(net_text)\n",
        "net_total_length"
      ],
      "metadata": {
        "id": "PA822OZSzS65"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def check_sent(word, sentences): \n",
        "    final = [all([w in x for w in word]) for x in sentences] \n",
        "    sent_len = [sentences[i] for i in range(0, len(final)) if final[i]]\n",
        "    return int(len(sent_len))"
      ],
      "metadata": {
        "id": "PvV6Xf88zvPX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tf_scoring(text,net_filtered_unique):\n",
        "  net_total_length=total_length(net_text)\n",
        "  tf_score = {}\n",
        "  for each_word in ' '.join([' '.join(i) for i in net_filtered_unique]).split(\" \"):\n",
        "      each_word = each_word.replace('.','')\n",
        "      if each_word not in stop_words:\n",
        "          if each_word in tf_score:\n",
        "              tf_score[each_word] += 1\n",
        "          else:\n",
        "              tf_score[each_word] = 1\n",
        "\n",
        "  # Dividing by total_word_length for each dictionary element\n",
        "  tf_score.update((x, y/int(np.mean(net_total_length))) for x, y in tf_score.items())\n",
        "  #print(tf_score)\n",
        "  return tf_score"
      ],
      "metadata": {
        "id": "zqi0dh64z9e7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_top_n(dict_elem, n):\n",
        "    result = dict(sorted(dict_elem.items(), key = itemgetter(1), reverse = True)[:n]) \n",
        "    return result"
      ],
      "metadata": {
        "id": "4ccAOHol0VMs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_power_words(net_text,net_filtered_unique):\n",
        "  keymax=[]\n",
        "  tf_val=get_top_n(tf_scoring(' '.join(net_text),net_filtered_unique), 20)\n",
        "  keymax.extend(sorted(tf_val, key=tf_val.get, reverse=True))\n",
        "  return list(set(list(keymax)))[:20]"
      ],
      "metadata": {
        "id": "Lxypt2Rc0Y-i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "power_words=get_power_words(net_text,net_filtered_unique)"
      ],
      "metadata": {
        "id": "7CUMeh9G7-eF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib\n",
        "import pandas\n",
        "import wordcloud\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "def plot_cloud(net_text):\n",
        "  text=' '.join(net_text)\n",
        "  comment_words = ''\n",
        "  stopwords = set(STOPWORDS)\n",
        "  net_filtered,net_filtered_unique=filtered_words(net_text)\n",
        "  # split the value\n",
        "  tokens = ' '.join(([' '.join(i) for i in net_filtered_unique])).split(\" \")\n",
        "\n",
        "  # Converts each token into lowercase\n",
        "  for i in range(len(tokens)):\n",
        "      tokens[i] = tokens[i].lower()\n",
        "\n",
        "      comment_words += \" \".join(tokens)+\" \"\n",
        "\n",
        "      wordcloud = WordCloud(width = 800, height = 800,background_color ='white',stopwords = stopwords,min_font_size= 10).generate(comment_words)\n",
        "  plt.figure(figsize = (8, 8), facecolor = None)\n",
        "  plt.imshow(wordcloud)\n",
        "  plt.axis(\"off\")\n",
        "  plt.tight_layout(pad = 0)\n",
        "  plt.show()\n",
        "  \n",
        "  return wordcloud"
      ],
      "metadata": {
        "id": "VldTXyhn8AbZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " plot_cloud(net_text).to_file(\"word_cloud.png\")"
      ],
      "metadata": {
        "id": "_Jm9uXNQZRmz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def power_word_density(net_text):\n",
        "  cou=[];\n",
        "  total_duration=net_total_duration\n",
        "  net_filtered,net_filtered_unique=filtered_words([val for key,val in ideal.items()])\n",
        "  for i in net_text:\n",
        "    for j in net_filtered_unique:\n",
        "        cou.append(len(j)/len(i.split(\" \")))\n",
        "  \n",
        "  cou=np.mean(cou)\n",
        "  #print('Power word density/min:',cou)\n",
        "  return cou"
      ],
      "metadata": {
        "id": "F5paUX4fZUTq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "power_density=power_word_density(net_text)\n",
        "power_density"
      ],
      "metadata": {
        "id": "TNg6tgULfM2z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import librosa\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import statsmodels.api as sm\n",
        "from scipy.signal import find_peaks\n",
        "import librosa.display\n",
        "import sklearn\n",
        "def pitch_calc(path):\n",
        "  ind=net_total_length.index(min(net_total_length))\n",
        "  path_to_wav=path[ind]\n",
        "  data, sampling_frequency = librosa.load(path_to_wav)\n",
        "  auto = sm.tsa.acf(data, nlags=2000)\n",
        "  peaks = find_peaks(auto)[0] # Find peaks of the autocorrelation\n",
        "  lag = peaks[0] # Choose the first peak as our pitch component lag\n",
        "  pitc=sampling_frequency / lag\n",
        "  return pitc,peaks"
      ],
      "metadata": {
        "id": "s6xv4h81fZQd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pitc,peaks=pitch_calc(path)"
      ],
      "metadata": {
        "id": "PIp2L6f0o0bp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pitc"
      ],
      "metadata": {
        "id": "r6vQGfnZo-ke"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "def volume_calc(path):\n",
        "  ind=net_total_length.index(min(net_total_length))\n",
        "  path_to_wav=path[ind]\n",
        "  y, sr = librosa.load(path_to_wav)\n",
        "  second = []\n",
        "  for s in range(0,len(y),sr):\n",
        "      second.append( np.abs(y[s:s+sr]).mean())\n",
        "  volume=[20 * math.log10(i) for i in second]\n",
        "  return np.max(np.abs(volume))"
      ],
      "metadata": {
        "id": "pBZMIdDtp3Qr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "volume=volume_calc(path)"
      ],
      "metadata": {
        "id": "A4U5y6Y2re1w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers\n",
        "from transformers import pipeline\n",
        "\n",
        "def sentiment_happy_proba(net_text):\n",
        "  happy_proba=[]\n",
        "  classifier = pipeline(\"text-classification\", model=\"j-hartmann/emotion-english-distilroberta-base\", return_all_scores=True)\n",
        "  for text in net_text:\n",
        "    classify=classifier(text)\n",
        "    df=pd.DataFrame(classify[0],columns=['label','score'])\n",
        "    happy_proba.append(float(df[df.label=='joy']['score']))\n",
        "  return happy_proba"
      ],
      "metadata": {
        "id": "VlqobpKxuU4d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "happy_proba=sentiment_happy_proba(net_text)\n",
        "happy_proba"
      ],
      "metadata": {
        "id": "UCun7enjuvJb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def summary(net_text,flag):\n",
        "  content_score=[]\n",
        "  flag=list(flag)\n",
        "  for i in range(len(flag)-1):\n",
        "    matchPercentage=0\n",
        "    mp2=0\n",
        "    if flag[i]=='Yes':\n",
        "      answer=ideal[(i+1)]\n",
        "      text=net_text[i]\n",
        "      text_list = [text,answer]\n",
        "      cv = CountVectorizer()\n",
        "      count_matrix = cv.fit_transform(text_list)\n",
        "      matchPercentage = cosine_similarity(count_matrix)[0][1] * 100\n",
        "      matchPercentage = round(matchPercentage, 2)\n",
        "      mp2=round(((fuzz.token_sort_ratio(text.lower(),answer.lower()))/len(answer.split(\" \")))*100)\n",
        "      content_score.append(mp2 if mp2>matchPercentage else matchPercentage)\n",
        "    else:\n",
        "      content_score.append('Not Applicable')\n",
        "  return content_score"
      ],
      "metadata": {
        "id": "XNDk2LYDu1AG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "content_score=summary(net_text,flag)"
      ],
      "metadata": {
        "id": "zM1LP4OL3c-o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pydub import AudioSegment,silence\n",
        "\n",
        "def silence_detect(path):\n",
        "  silence_secs=[]\n",
        "  silence_num=[]\n",
        "  for i in path:\n",
        "    myaudio = intro = AudioSegment.from_wav(i)\n",
        "    dBFS=myaudio.dBFS\n",
        "    silenced = silence.detect_silence(myaudio, min_silence_len=1000, silence_thresh=dBFS-16)\n",
        "\n",
        "    silenced = [((stop/1000)-(start/1000)) for start,stop in silenced] #in sec\n",
        "    #print(np.max(silenced),len(silenced))\n",
        "    if silenced!=[]:\n",
        "      silence_secs.append(np.max(silenced)),silence_num.append(len(silenced))\n",
        "    else:\n",
        "      silence_secs.append(0),silence_num.append(0)\n",
        "  return silence_secs,silence_num"
      ],
      "metadata": {
        "id": "C4pE_MR33-ES"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "silence_secs,silence_num=silence_detect(net_transcribed_audio_file_name)"
      ],
      "metadata": {
        "id": "C070F5H98FHz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def filler_words_cohort(net_text):\n",
        "  cohort=[]\n",
        "  net_filtered,net_filtered_unique=filtered_words(net_text)\n",
        "  for i in range(len(net_filtered)):\n",
        "    cohort_score=len(net_filtered[i])/len(net_text[i].split(\" \"))\n",
        "    if cohort_score>0.7:\n",
        "      cohort.append(4)\n",
        "    elif cohort_score>0.4:\n",
        "      cohort.append(3)\n",
        "    elif cohort_score>0.1:\n",
        "      cohort.append(2)\n",
        "    else:\n",
        "      cohort.append(1)\n",
        "  return cohort"
      ],
      "metadata": {
        "id": "jVdZsnpT86eX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filler_words=filler_words_cohort(net_text)\n",
        "filler_words"
      ],
      "metadata": {
        "id": "aqS0AxhL-NYc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sentiment_cohort(net_text):\n",
        "  happy_proba=sentiment_happy_proba(net_text)\n",
        "  happy=[]\n",
        "  for i in happy_proba: \n",
        "    if i>0.7:\n",
        "      happy.append(4)\n",
        "    elif i>0.4:\n",
        "      happy.append(3)\n",
        "    elif i>0.1:\n",
        "      happy.append(2)\n",
        "    else:\n",
        "      happy.append(1)\n",
        "  return happy"
      ],
      "metadata": {
        "id": "apONWGFi-gpr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment=sentiment_cohort(net_text)"
      ],
      "metadata": {
        "id": "EqEO3C09_fd_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def confidence_cohort(path):\n",
        "  net=[]\n",
        "  net_text,net_word_count,net_total_duration,net_transcribed_audio_file_name=audio_read(path)\n",
        "  filler_words=filler_words_cohort(net_text)\n",
        "  sentiment=sentiment_cohort(net_text)\n",
        "  for i in range(len(sentiment)):\n",
        "    cohort=(sentiment[i]+filler_words[i])/2\n",
        "    cohort=int(cohort)\n",
        "    if cohort==1:\n",
        "      net.append('D')\n",
        "    elif cohort==2:\n",
        "      net.append('C')\n",
        "    elif cohort==3:\n",
        "      net.append('B')\n",
        "    elif cohort==4:\n",
        "      net.append('A')\n",
        "  return net"
      ],
      "metadata": {
        "id": "63M6Z3Nc_r7y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "confidence=confidence_cohort(path)"
      ],
      "metadata": {
        "id": "q6codCjfBkYa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pitch_volume_cohort(path):\n",
        "  pitc,peaks=pitch_calc(path)\n",
        "  volume=volume_calc(path)\n",
        "  pitch=np.abs(pitc-670)\n",
        "  p_cohort=0\n",
        "  v_cohort=0\n",
        "  if pitch<=500:\n",
        "    p_cohort=4\n",
        "  elif pitch<=1000:\n",
        "    p_cohort=3\n",
        "  elif pitch<=1500:\n",
        "    p_cohort=2\n",
        "  else:\n",
        "    p_cohort=1\n",
        "  if volume>=75:\n",
        "    v_cohort=4\n",
        "  elif volume>=50:\n",
        "    v_cohort=3\n",
        "  elif volume>=35:\n",
        "    v_cohort=2\n",
        "  else:\n",
        "    v_cohort=1\n",
        "  return math.ceil((p_cohort+v_cohort)/2)"
      ],
      "metadata": {
        "id": "xTxaXfvlBqCd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pitch_volum=pitch_volume_cohort(path)\n",
        "pitch_volum"
      ],
      "metadata": {
        "id": "17NWZcSwCSBO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def energy_cohort(path):\n",
        "  cohort= pitch_volume_cohort(path)\n",
        "  cohort=int(cohort)\n",
        "  cohort=[cohort]*6\n",
        "  net=[]\n",
        "  for i in range(len(cohort)):\n",
        "    if cohort[i]==1:\n",
        "      net.append('D')\n",
        "    elif cohort[i]==2:\n",
        "      net.append('C')\n",
        "    elif cohort[i]==3:\n",
        "      net.append('B')\n",
        "    elif cohort[i]==4:\n",
        "      net.append('A')\n",
        "  return net"
      ],
      "metadata": {
        "id": "Qo11dOICCXKK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "energy=energy_cohort(path)"
      ],
      "metadata": {
        "id": "fO6zTfWUD-vE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fluency_cohort(path):\n",
        "  net_text,net_word_count,net_total_duration,net_transcribed_audio_file_name=audio_read(path)\n",
        "  net_filtered,net_filtered_unique=filtered_words(net_text)\n",
        "  tex=''\n",
        "  pace_cohort=[]\n",
        "  for i in range(len(net_filtered)):\n",
        "    if net_word_count[i]>170 and net_word_count[i]<190:\n",
        "      pace_cohort.append(4)\n",
        "    elif net_word_count[i]>100 and net_word_count[i]<120:\n",
        "      pace_cohort.append(3)\n",
        "    elif net_word_count[i]>50 and net_word_count[i]>90:\n",
        "      pace_cohort.append(2)\n",
        "    else:\n",
        "      pace_cohort.append(1)\n",
        "  silenc,coun=silence_detect(net_transcribed_audio_file_name)\n",
        "  silence_cohort=[]\n",
        "  for i in range(len(silenc)):\n",
        "    if silenc[i] <3 or coun[i]<4:\n",
        "      silence_cohort.append(4)\n",
        "    elif silenc[i] <5 or coun[i]<8:\n",
        "      silence_cohort.append(3)\n",
        "    elif silenc[i] < 8 or coun[i]<10:\n",
        "      silence_cohort.append(2)\n",
        "    else:\n",
        "      silence_cohort.append(1)\n",
        "  net=[]\n",
        "  for i in range(len(pace_cohort)):\n",
        "    coh=(pace_cohort[i]+silence_cohort[i])//2\n",
        "    if coh==4:\n",
        "      net.append('A')\n",
        "    elif coh==3:\n",
        "      net.append('B')\n",
        "    elif coh==2:\n",
        "      net.append('C')\n",
        "    elif coh==1:\n",
        "      net.append('D')\n",
        "  return net "
      ],
      "metadata": {
        "id": "vadN0kkwECRu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fluency=fluency_cohort(path)"
      ],
      "metadata": {
        "id": "lJil2eYaNO0D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fluency"
      ],
      "metadata": {
        "id": "EVoTIBaLNX7Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def aggregate_score(content_score,confidence,energy,fluency):\n",
        "  content_score=[i for i in content_score if i!='Not Applicable']\n",
        "  content_score=np.mean(content_score)\n",
        "  cohort=0\n",
        "  net=[]\n",
        "  if content_score>20 and content_score<30:\n",
        "    cohort=1\n",
        "  elif content_score>30 and content_score<40:\n",
        "    cohort=2\n",
        "  elif content_score>40 and content_score<60:\n",
        "    cohort=3\n",
        "  elif content_score>60 and content_score<70:\n",
        "    cohort=4\n",
        "  for i in range(len(confidence)):\n",
        "    net.append(((69-ord(confidence[i]))+(69-ord(energy[i]))+(69-ord(fluency[i]))+cohort)//4)\n",
        "  return [chr(69-i)for i in net]"
      ],
      "metadata": {
        "id": "nicwTv6kOt6P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "content_score"
      ],
      "metadata": {
        "id": "T3qdN_TuOfVO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "[ord(i) for i in confidence]"
      ],
      "metadata": {
        "id": "gDhJNoPuQ7Ty"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fluency"
      ],
      "metadata": {
        "id": "kBdyYhg1Q9Wj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "energy"
      ],
      "metadata": {
        "id": "J36uNWBXQ_rx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "per_ques=aggregate_score(content_score,confidence,energy,fluency)\n",
        "per_ques"
      ],
      "metadata": {
        "id": "c96dMZXxQUCS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "interaction_score=69-np.mean([ord(i) for i in per_ques])\n",
        "interaction_score"
      ],
      "metadata": {
        "id": "j2-Kv65qQqs5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "interaction_cohort=chr(69-round(interaction_score))\n",
        "interaction_cohort"
      ],
      "metadata": {
        "id": "Qh1H8IP5TdDj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "quotients=pd.read_csv(\"/content/The Power words dashboard  - Power Words.csv\").dropna()\n",
        "text=' '.join(net_text)"
      ],
      "metadata": {
        "id": "9h737cybV9Hf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sales=' '.join(list(quotients['Sales Power words ']))\n",
        "manager=' '.join(list(quotients['Manager Power Words ']))\n",
        "leadership=' '.join(list(quotients['Leadership Power Words']))\n",
        "hr=' '.join(list(quotients['HR Power Words ']))\n",
        "learning=' '.join(list(quotients['Learning Quotient ']))\n",
        "service=' '.join(list(quotients['Service Quotient ']))"
      ],
      "metadata": {
        "id": "HZAOF2aKhuy5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def quotient_scores(sales,manager,leadership,hr,learning,service,text):\n",
        "  content_score={}\n",
        "  sales_list = [text,sales]\n",
        "  mg_list=[text,manager]\n",
        "  ld_list=[text,leadership]\n",
        "  hr_list=[text,hr]\n",
        "  lr_list=[text,learning]\n",
        "  sv_list=[text,service]\n",
        "  cv = CountVectorizer()\n",
        "  count_matrix = cv.fit_transform(sales_list)\n",
        "  matchPercentage = cosine_similarity(count_matrix)[0][1] * 100\n",
        "  matchPercentage = round(matchPercentage, 2)\n",
        "  mp2=round(((fuzz.token_sort_ratio(text.lower(),sales.lower()))/len(sales.split(\" \")))*100)\n",
        "  content_score['sales']=(mp2 if mp2>matchPercentage else matchPercentage)\n",
        "\n",
        "  count_matrix = cv.fit_transform(mg_list)\n",
        "  matchPercentage = cosine_similarity(count_matrix)[0][1] * 100\n",
        "  matchPercentage = round(matchPercentage, 2)\n",
        "  mp2=round(((fuzz.token_sort_ratio(text.lower(),manager.lower()))/len(manager.split(\" \")))*100)\n",
        "  content_score['manager']=(mp2 if mp2>matchPercentage else matchPercentage)\n",
        "\n",
        "  count_matrix = cv.fit_transform(ld_list)\n",
        "  matchPercentage = cosine_similarity(count_matrix)[0][1] * 100\n",
        "  matchPercentage = round(matchPercentage, 2)\n",
        "  mp2=round(((fuzz.token_sort_ratio(text.lower(),leadership.lower()))/len(leadership.split(\" \")))*100)\n",
        "  content_score['leadership']=(mp2 if mp2>matchPercentage else matchPercentage)\n",
        "\n",
        "  count_matrix = cv.fit_transform(hr_list)\n",
        "  matchPercentage = cosine_similarity(count_matrix)[0][1] * 100\n",
        "  matchPercentage = round(matchPercentage, 2)\n",
        "  mp2=round(((fuzz.token_sort_ratio(text.lower(),hr.lower()))/len(hr.split(\" \")))*100)\n",
        "  content_score['hr']=(mp2 if mp2>matchPercentage else matchPercentage)\n",
        "  \n",
        "  count_matrix = cv.fit_transform(lr_list)\n",
        "  matchPercentage = cosine_similarity(count_matrix)[0][1] * 100\n",
        "  matchPercentage = round(matchPercentage, 2)\n",
        "  mp2=round(((fuzz.token_sort_ratio(text.lower(),learning.lower()))/len(learning.split(\" \")))*100)\n",
        "  content_score['learning']=(mp2 if mp2>matchPercentage else matchPercentage)\n",
        "\n",
        "  count_matrix = cv.fit_transform(sv_list)\n",
        "  matchPercentage = cosine_similarity(count_matrix)[0][1] * 100\n",
        "  matchPercentage = round(matchPercentage, 2)\n",
        "  mp2=round(((fuzz.token_sort_ratio(text.lower(),service.lower()))/len(service.split(\" \")))*100)\n",
        "  content_score['service']=(mp2 if mp2>matchPercentage else matchPercentage)\n",
        "\n",
        "  return content_score\n"
      ],
      "metadata": {
        "id": "8DjXI9s8i722"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "quotient_dict=quotient_scores(sales,manager,leadership,hr,learning,service,text)"
      ],
      "metadata": {
        "id": "gzeiM1mijduq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "quotient_dict"
      ],
      "metadata": {
        "id": "tKhZZCrrlGCy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "id"
      ],
      "metadata": {
        "id": "Yjtv9ncOmNmy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab.patches import cv2_imshow\n",
        "import cv2\n",
        "cloud=cv2.imread('/content/word_cloud.png')"
      ],
      "metadata": {
        "id": "Bv538l4hqCSF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "content_score=[i for i in content_score if i!='Not Applicable']\n",
        "content_score=np.mean(content_score)"
      ],
      "metadata": {
        "id": "exPWq76rt7CO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Grammer Score INIT"
      ],
      "metadata": {
        "id": "wr7LPp2J9Zic"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "percentage_grammer_score , avg_grammer_score = Grammer_score(net_text)"
      ],
      "metadata": {
        "id": "2u8YCFwf9R9F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ZW5hbRUq9X3y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#fill up ids\n",
        "output={}\n",
        "output['Candidate_ID']=[id['Candidate_ID']]*6\n",
        "output['Interaction_ID']=[id['Interaction_ID']]*6\n",
        "output['Test_ID']=[id['Test_ID']]*6\n",
        "output['Interaction_Mode']=[df.iloc[0]['Interaction_Mode']]*6\n",
        "output['MCQ_Percentage']=[(0 if output['Interaction_Mode']!='MCQ' else 1)]*6\n",
        "output['Sales_Quotient']=[quotient_dict['sales']]*6\n",
        "output['Manager_Quotient']=[quotient_dict['manager']]*6\n",
        "output['Leadership_Quotient']=[quotient_dict['leadership']]*6\n",
        "output['Learner_Quotient']=[quotient_dict['learning']]*6\n",
        "output['People_Quotient']=[quotient_dict['service']]*6\n",
        "output['Resume_Score']=[resume_sc]*6\n",
        "output['Pace']=[np.mean(net_word_count)]*6\n",
        "output['Power_word_density']=[power_density]*6\n",
        "output['Word_Cloud']=[cloud]*6\n",
        "output['Volume']=[volume]*6\n",
        "output['Pitch']=[pitc]*6\n",
        "output['Aggregate_content_score']=[content_score]*6\n",
        "output['Raw_interaction_score']=[interaction_score]*6\n",
        "output['Interaction_score']=[interaction_cohort]*6\n",
        "output['Question_no.']=list(df['Question_ID'])[:-1]\n",
        "output['MCQ_Value']=[0]*6\n",
        "#pd.DataFrame(output,columns=output.keys(),index=[0])\n",
        "\n",
        "#avg Grammer score\n",
        "output['MCQ_Value']=[avg_grammer_score]*6"
      ],
      "metadata": {
        "id": "Oc3p14zalIJl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Grammer output['Question_no.']"
      ],
      "metadata": {
        "id": "0SAJmLJuBS4k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "content_score"
      ],
      "metadata": {
        "id": "6mpGU6IPCLD6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output['Transcript']=net_text\n",
        "#percentage grammer score\n",
        "output['Transcript_Grammer_Score']= percentage_grammer_score \n",
        "output['Confidence']=confidence\n",
        "output['Fluency']=fluency\n",
        "output['Energy']=energy\n",
        "output['Content_score']=content_score\n",
        "output['per_question_score']=per_ques\n",
        "output['silence_number'],output['silence_length']=silence_secs,silence_num\n",
        "output['filler_words_score']=filler_words_cohort(net_text)\n",
        "output['sentiment_score']=happy_proba"
      ],
      "metadata": {
        "id": "UUCzxxjVwrJ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import collections.abc\n",
        "\n",
        "for key,val in output.items():\n",
        "  if isinstance(val, collections.abc.Sequence):\n",
        "    print(key,len(val))\n",
        "  else:\n",
        "    print('no len for ',key)"
      ],
      "metadata": {
        "id": "oqaR6AF3BVZQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output=pd.DataFrame(output,columns=output.keys())"
      ],
      "metadata": {
        "id": "7dH_Yqny712O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cv2_imshow(output['Word_Cloud'][0])"
      ],
      "metadata": {
        "id": "2FtRno9t9Mvr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output"
      ],
      "metadata": {
        "id": "ORRAI6reCww6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jcIuyBV759nH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}